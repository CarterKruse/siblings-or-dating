{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that InceptionResnetV1 is arguably one of the most primitive models. This is what ChatGPT says:\n",
    "\n",
    "Beyond InceptionResNetV1, there are several variants of FaceNet models, each designed with different architectures and optimizations. FaceNet itself is a framework for face recognition, and it can be built on various network architectures. Some of the most well-known FaceNet variants include:\n",
    "\n",
    "Inception-ResNet-V2: A more advanced version of InceptionResNetV1, this model improves performance through a better balance of Inception networks and residual connections.\n",
    "ResNet-based FaceNet: Some variations of FaceNet are built on ResNet architectures, such as ResNet-50, ResNet-101, or ResNet-152. These models are designed to handle more complex datasets and have better feature extraction capabilities.\n",
    "MobileNet-based FaceNet: Designed for efficiency, MobileNet architectures are lightweight models that are optimized for mobile and edge devices. MobileNet-based FaceNet models are designed for real-time applications with limited computational resources.\n",
    "EfficientNet-based FaceNet: EfficientNet architectures provide high accuracy with fewer parameters. They are designed to be more efficient in terms of computation and memory, which can be important for deployment on resource-constrained devices.\n",
    "VGG-based FaceNet: VGG networks have also been adapted for face recognition tasks. They are simpler but highly effective in many applications, especially when fine-tuned for specific tasks.\n",
    "Siamese Networks: In some cases, FaceNet models are implemented as Siamese Networks (pairwise models) to enhance performance in one-shot learning scenarios, where the model needs to recognize faces with only a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacePairDataset(Dataset):\n",
    "    def __init__(self, image1_paths, image2_paths, labels, transform=None):\n",
    "        self.image1_paths = image1_paths\n",
    "        self.image2_paths = image2_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.mtcnn = MTCNN(image_size=160, keep_all=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image1 = Image.open(self.image1_paths[idx]).convert('RGB')\n",
    "        image2 = Image.open(self.image2_paths[idx]).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Detect faces and align them (using MTCNN)\n",
    "        faces1, _ = self.mtcnn(image1, return_prob=True)\n",
    "        faces2, _ = self.mtcnn(image2, return_prob=True)\n",
    "\n",
    "        if faces1 is None or faces2 is None:\n",
    "            raise ValueError('No faces detected in one or both images.')\n",
    "        \n",
    "        face1 = faces1[0]\n",
    "        face2 = faces2[0]\n",
    "\n",
    "        if self.transform:\n",
    "            face1 = self.transform(face1)\n",
    "            face2 = self.transform(face2)\n",
    "\n",
    "        return (face1, face2), torch.tensor(label, dtype=torch.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        # Compute the cosine similarity between the two embeddings\n",
    "        similarity = F.cosine_similarity(output1, output2)\n",
    "        # Contrastive loss function: penalize high similarity for different faces\n",
    "        loss = torch.mean((1 - label) * torch.pow(similarity, 2) + (label) * torch.pow(torch.clamp(1 - similarity, min=0.0), 2))\n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTunedFaceNet(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(FineTunedFaceNet, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        # Add a fully connected layer to produce a fixed-size embedding vector (e.g. 128-dimensional)\n",
    "        self.fc = nn.Linear(base_model.last_linear.in_features, 128)    # 128-dimensional embedding vector\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the base model\n",
    "        x = self.base_model(x)\n",
    "        # Pass through a fully connected layer to get the final embedding\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load in a dataset\n",
    "# Create combinations \n",
    "\n",
    "\n",
    "image1_paths = []\n",
    "image2_paths = []\n",
    "labels = [1, 0]\n",
    "\n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((160, 160)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create the dataset and dataloaders\n",
    "dataset = FacePairDataset(image1_paths, image2_paths, labels, transform)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "base_model = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "model = FineTunedFaceNet\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for (face1, face2), label in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: get embeddings for the two face images\n",
    "        embedding1 = model(face1)\n",
    "        embedding2 = model(face2)\n",
    "\n",
    "        # Compute the contrastive loss\n",
    "        loss = ContrastiveLoss()(embedding1, embedding2, label)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'finetuned_facenet.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pleasework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
